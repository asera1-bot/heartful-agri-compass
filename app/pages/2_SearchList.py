import os
import sys

import pandas as pd
import streamlit as st
from sqlalchemy.exc import OperationalError

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
APP_DIR = os.path.dirname(CURRENT_DIR)
ROOT_DIR = os.path.dirname(APP_DIR)

if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)


from core.auth import require_login
from core.db import get_engine, DB_PATH

require_login()

st.title("Search / List")
st.caption("åŽé‡ãƒ‡ãƒ¼ã‚¿ã‚’æ¡ä»¶ã§æ¤œç´¢ã—ã€ä¸€è¦§è¡¨ç¤ºãƒ»CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")

engine = get_engine()

# DBèª­ã¿è¾¼ã¿
with st.spinner("åŽé‡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚"):
    try:
        with engine.connect() as conn:
            df = pd.read_sql_query(
                """
                select
                    date(harvest_date) as harvest_date,
                    company,
                    crop,
                    amount_kg
                from harvest_fact
                order by harvest_date
                """,
                conn,
            )

            df["harvest_date"] = pd.to_datetime(df["harvest_date"], errors="coerce")

    except OperationalError as e:
        st.error("SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŽ¥ç¶šã§ãã¾ã›ã‚“ã€‚")
        st.code(str(DB_PATH), language="bash")
        st.exception(e)
        st.stop()

if df.empty:
    st.warning("åŽé‡ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¾ã›ã‚“ã€‚CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

df["harvest_date"] = pd.to_datetime(df["harvest_date"])

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€€UI
st.subheader("æ¤œç´¢æ¡ä»¶")

min_date = df["harvest_date"].min().date()
max_date = df["harvest_date"].max().date()

date_start, date_end = st.date_input(
    "å¯¾è±¡æœŸé–“",
    (min_date, max_date),
    min_value=min_date,
    max_value=max_date,
)

all_companies = sorted(df["company"].unique().tolist())
company_filter = st.multiselect(
    "ä¼æ¥­ï¼ˆæœªé¸æŠžãªã‚‰å…¨ä»¶ï¼‰",
    options=all_companies,
    default=[],
)

all_crops = sorted(df["crop"].unique().tolist())
crop_filter = st.multiselect(
    "ä½œç‰©ï¼ˆæœªé¸æŠžãªã‚‰å…¨ä»¶ï¼‰",
    options=all_crops,
    default=[],
)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
filtered = df.copy()

filtered = filtered[
    (filtered["harvest_date"].dt.date >= date_start)
    & (filtered["harvest_date"].dt.date <= date_end)
]

if company_filter:
    filtered = filtered[filtered["company"].isin(company_filter)]

if crop_filter:
    filtered = filtered[filtered["crop"].isin(crop_filter)]

hit_count = len(filtered)

st.markdown("### ðŸ”ã€€æ¤œç´¢æ¡ä»¶")
st.subheader(f"æ¤œç´¢çµæžœ:{hit_count}è¡Œ")

if hit_count == 0:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

# ä¸€è¦§è¡¨ç¤ºã€€ï¼†ã€€CDVã€€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
st.dataframe(
    filtered.sort_values(["harvest_date", "company", "crop"]),
    width="stretch",)

st.success(f"{len(filtered)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒãƒ’ãƒƒãƒˆã—ã¾ã—ãŸã€‚")

# CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
csv_bytes = filtered.to_csv(index=False).encode("utf-8-sig")

st.download_button(
    label="æ¤œç´¢çµæžœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_bytes,
    file_name="harvest_search_result.csv",
    mime="text/csv",
)

st.write("ã“ã“ã«åŽé‡ãƒ‡ãƒ¼ã‚¿ãªã©ã®æ¤œç´¢ãƒ»ä¸€è¦§ç”»é¢ã‚’å®Ÿè£…ã—ã¦ã„ãã¾ã™ã€‚")

# ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ25ä»¶ãšã¤ï¼‰
page_size = 25
max_page = max(1, (len(filtered) + page_size - 1) // page_size)

if "page" not in st.session_state:
    st.session_state.page = 1

col_prev, col_mid, col_next = st.columns([1, 2, 1])

with col_prev:
    if st.button("â†ã€€å‰") and st.session_state.page > 1:
        st.session_state.page -= 1

with col_mid:
    st.write(f"ãƒšãƒ¼ã‚¸{st.session_state.page} / {max_page}")

with col_next:
    if st.button("æ¬¡ã€€â†’") and st.session_state.page < max_page:
        st.session_state.page += 1

start = (st.session_state.page - 1) * page_size
end = start + page_size

st.dataframe(
        filtered.sort_values(["harvest_date", "company", "crop"]).iloc[start:end],
        use_container_width=True,)
